{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "<div style=\"text-align: right\"> University of Tübingen, Summer Term 2023  &copy; 2023 P. Hennig </div>\n",
    "\n",
    "## Exercise Sheet No. 3 — Exponential Families\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Submission by:\n",
    "* FirstName1, Surname1, Matrikelnummer: MatrikelnummerOfFirstTeamMember\n",
    "* FirstName2, Surname2, Matrikelnummer: MatrikelnummerOfSecondTeamMember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from tueplots import bundles\n",
    "from tueplots.constants.color import rgb\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({\"figure.dpi\": 200})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 (Coding Exercise)\n",
    "\n",
    "Consider the abstract base class `ExponentialFamily` introduced in the lecture (reproduced below for easy reference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import functools\n",
    "\n",
    "\n",
    "class ExponentialFamily(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def sufficient_statistics(self, x: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_base_measure(self, x: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_partition(self, parameters: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\"\"\"\n",
    "\n",
    "    def parameters_to_natural_parameters(\n",
    "        self, parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->(P)`\n",
    "        In some EF's, the canonical parameters are\n",
    "        actually a transformation of the natural parameters.\n",
    "        In such cases, this method should be overwritten to\n",
    "        provide the inverse transformation.\n",
    "        \"\"\"\n",
    "        return jnp.asarray(parameters)\n",
    "\n",
    "    def logpdf(\n",
    "        self, x: ArrayLike | jnp.ndarray, parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        log p(x|parameters)\n",
    "            = log h(x) + sufficient_statistics(x) @ natural_parameters - log Z(natural_parameters)\n",
    "            = log base measure + linear term - log partition\n",
    "        \"\"\"\n",
    "\n",
    "        x = jnp.asarray(x)\n",
    "        log_base_measure = self.log_base_measure(x)\n",
    "        natural_parameters = self.parameters_to_natural_parameters(parameters)\n",
    "        linear_term = (\n",
    "            self.sufficient_statistics(\n",
    "                x)[..., None, :] @ natural_parameters[..., None]\n",
    "        )[..., 0, 0]\n",
    "        log_partition = self.log_partition(parameters)\n",
    "\n",
    "        return log_base_measure + linear_term - log_partition\n",
    "\n",
    "    def conjugate_log_partition(\n",
    "        self, alpha: ArrayLike | jnp.ndarray, nu: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"The log partition function of the conjugate exponential family.\n",
    "        Signature `(P),()->()`\n",
    "        If(!) this is available, it allows analytic construction of the conjugate prior (and thus analytic posterior inference).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def conjugate_prior(self) -> \"ConjugateFamily\":\n",
    "        return ConjugateFamily(self)\n",
    "\n",
    "    def predictive_log_marginal_pdf(\n",
    "        self,\n",
    "        x: ArrayLike | jnp.ndarray,\n",
    "        conjugate_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\" Signature `(D),(P)->()`\n",
    "            log p(x|conjugate_natural_parameters)\n",
    "            Your answer to Part B below should be implemented here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def Laplace_predictive_log_marginal_pdf(\n",
    "        self,\n",
    "        x: ArrayLike | jnp.ndarray,\n",
    "        conjugate_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        mode: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\" Signature `(D),(P)->()`\n",
    "            log p(x|conjugate_natural_parameters)\n",
    "            Your answer to Part B below should be implemented here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def posterior_parameters(\n",
    "        self,\n",
    "        prior_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        data: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Computes the natural parameters of the posterior distribution under the\n",
    "        conjugate prior.\n",
    "        Signature `(P),(D)->(P)`\n",
    "        This can be implemented already in the abc and inherited by all subclasses,\n",
    "        even if the conjugate log partition function is not available.\n",
    "        (In the latter case, only the unnormalized posterior is immediately available, see below).\n",
    "        \"\"\"\n",
    "\n",
    "        prior_natural_parameters = jnp.asarray(prior_natural_parameters)\n",
    "\n",
    "        sufficient_statistics = self.sufficient_statistics(data)\n",
    "\n",
    "        n = sufficient_statistics[..., 0].size\n",
    "        expected_sufficient_statistics = jnp.sum(\n",
    "            sufficient_statistics,\n",
    "            axis=tuple(range(sufficient_statistics.ndim)),\n",
    "        )\n",
    "\n",
    "        alpha_prior, nu_prior = (\n",
    "            prior_natural_parameters[:-1],\n",
    "            prior_natural_parameters[-1],\n",
    "        )\n",
    "\n",
    "        return jnp.append(alpha_prior + expected_sufficient_statistics, nu_prior + n)\n",
    "\n",
    "\n",
    "class ConjugateFamily(ExponentialFamily):\n",
    "    def __init__(self, likelihood: ExponentialFamily) -> None:\n",
    "        self._likelihood = likelihood\n",
    "\n",
    "    @functools.partial(jnp.vectorize, excluded={0}, signature=\"(d)->(p)\")\n",
    "    def sufficient_statistics(self, w: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\n",
    "        the sufficient statistics of the conjugate family are\n",
    "        the natural parameters and the (negative) log partition function of the likelihood.\n",
    "        \"\"\"\n",
    "        return jnp.append(\n",
    "            self._likelihood.parameters_to_natural_parameters(w),\n",
    "            -self._likelihood.log_partition(w),\n",
    "        )\n",
    "\n",
    "    def log_base_measure(self, w: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\n",
    "        the base measure of the conjugate family is, implicitly, the Lebesgue measure.\n",
    "        \"\"\"\n",
    "        w = jnp.asarray(w)\n",
    "\n",
    "        return jnp.zeros_like(w[..., 0])\n",
    "\n",
    "    def log_partition(\n",
    "        self, natural_parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\n",
    "        If the conjugate log partition function is available,\n",
    "        we can use it to compute the log partition function of the conjugate family.\n",
    "        \"\"\"\n",
    "        natural_parameters = jnp.asarray(natural_parameters)\n",
    "\n",
    "        alpha, nu = natural_parameters[:-1], natural_parameters[-1]\n",
    "\n",
    "        return self._likelihood.conjugate_log_partition(alpha, nu)\n",
    "\n",
    "    def unnormalized_logpdf(\n",
    "        self, w: ArrayLike | jnp.ndarray, natural_parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        Even if the conjugate log partition function is not available,\n",
    "        we can still compute the unnormalized log pdf of the conjugate family.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.sufficient_statistics(w) @ jnp.asarray(natural_parameters)\n",
    "\n",
    "    def laplace_precision(\n",
    "        self,\n",
    "        natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        mode: ArrayLike | jnp.ndarray,\n",
    "        /,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P),(D)->()`\n",
    "        If the conjugate log partition function is _not_ available,\n",
    "        we can still compute the Laplace approximation to the posterior,\n",
    "        using only structure provided by the likelihood.\n",
    "        This requires the mode of the likelihood, which is not available in general,\n",
    "        but may be found by numerical optimization if necessary.\n",
    "        \"\"\"\n",
    "        return -jax.hessian(self.unnormalized_logpdf, argnums=0)(\n",
    "            jnp.asarray(mode), natural_parameters\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task A.** \n",
    "\n",
    "Implement a concrete realization of the binomial exponential family parametrized by log odds ratio $w = \\log \\frac{p}{1 - p}$, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "    p(k \\mid w) = \\exp \\left(\\log h(k) + \\phi(k)^T w - \\log Z(w) \\right),\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "* $\\log h(k) := \\log \\binom{n}{k}$,\n",
    "* $\\phi(k) := k$, and\n",
    "* $\\log Z(w) := n \\log (1 + \\exp(w))$.\n",
    "\n",
    "(Note that $n$ is a constant in this definition, not a parameter). The normalization constant of the conjugate family\n",
    "\n",
    "\\begin{align*}\n",
    "    F(\\alpha, \\nu)\n",
    "    & := \\int_{-\\infty}^\\infty \\exp \\left( \\alpha w - \\nu \\log Z(w) \\right) \\mathrm{d}w \\\\\n",
    "    & = \\int_{-\\infty}^\\infty \\exp \\left( w \\right)^\\alpha \\left( 1 + \\exp(w) \\right)^{-n \\nu} \\mathrm{d}w \\\\\n",
    "    & = \\int_0^1 \\left( \\frac{p}{1 - p} \\right)^\\alpha \\left( 1 + \\frac{p}{1 - p} \\right)^{-n \\nu} \\left| \\frac{1}{p (1 - p)} \\right| \\mathrm{d}p \\\\\n",
    "    & = \\int_0^1 p^{\\alpha - 1} (1 - p)^{(n \\nu - \\alpha) - 1} \\mathrm{d}p \\\\\n",
    "    & = B(\\alpha, n \\nu - \\alpha),\n",
    "\\end{align*}\n",
    "\n",
    "since $p = \\frac{1}{1 + \\exp(-w)}$ and $\\frac{\\mathrm{d} p}{\\mathrm{d} w} = \\frac{\\exp(-w)}{(1 + \\exp(-w))^2} = p (1 - p)$.\n",
    "This is also the normalization constant of the type VI logistic or logistic-beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus, the following transformation is a useful utility:\n",
    "def sigmoid_logpdf_transform(logpdf_logodds):\n",
    "    \"\"\"Transform the log-pdf of a random variable X into the\n",
    "    log-pdf of the random variable sigmoid(X)\"\"\"\n",
    "\n",
    "    def logpdf_p(ps):\n",
    "        logps = jnp.log(ps)\n",
    "        log1mps = jnp.log1p(-ps)\n",
    "        logodds = logps - log1mps\n",
    "\n",
    "        return logpdf_logodds(logodds) - logps - log1mps\n",
    "\n",
    "    return logpdf_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your implementation of the Binomial distribution ###\n",
    "# class BinomialLogOdds(ExponentialFamily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some unit tests to make sure your implementation is correct:\n",
    "# instantiate your EF, and its conjugate prior:\n",
    "likelihood = BinomialLogOdds(n=1)\n",
    "prior = likelihood.conjugate_prior()\n",
    "\n",
    "a, b = 0.5, 0.5\n",
    "prior_natural_parameters = [\n",
    "    a,  # alpha\n",
    "    a + b,  # nu\n",
    "]  # => Logistic-Beta(a, b)\n",
    "\n",
    "# create some data:\n",
    "key = jax.random.PRNGKey(0)\n",
    "data = jax.random.bernoulli(key, 0.75, shape=(20, 1))\n",
    "\n",
    "posterior = prior\n",
    "posterior_natural_parameters = likelihood.posterior_parameters(\n",
    "    prior_natural_parameters,\n",
    "    data,\n",
    ")\n",
    "\n",
    "\n",
    "# A: Check your implementation of the conjugate prior is correctly normalized:\n",
    "import scipy.integrate\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    scipy.integrate.quad(\n",
    "        lambda logodds: np.exp(prior.logpdf(\n",
    "            [logodds], prior_natural_parameters)),\n",
    "        -30,\n",
    "        30,\n",
    "    )[0],\n",
    "    1.0,\n",
    "    rtol=1e-5,\n",
    "    err_msg=\"The conjugate prior is not correctly normalized.\",\n",
    ")\n",
    "\n",
    "# B: check your log pdf against the scipy implementation:\n",
    "fig, axs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "plt_ps = np.linspace(0.0, 1.0, 100)\n",
    "\n",
    "# first for the prior:\n",
    "axs[0].plot(\n",
    "    plt_ps,\n",
    "    jnp.exp(\n",
    "        sigmoid_logpdf_transform(\n",
    "            lambda logodds: prior.logpdf(\n",
    "                logodds[..., None], prior_natural_parameters)\n",
    "        )(plt_ps[..., None])\n",
    "    ), \n",
    "    label='my implementation'\n",
    ")\n",
    "\n",
    "axs[0].plot(plt_ps, jax.scipy.stats.beta.pdf(plt_ps, a, b),'--', label='scipy')\n",
    "axs[0].set_xlabel(r\"$p$\")\n",
    "\n",
    "# then for the posterior:\n",
    "axs[1].plot(\n",
    "    plt_ps,\n",
    "    jnp.exp(\n",
    "        sigmoid_logpdf_transform(\n",
    "            lambda logodds: posterior.logpdf(\n",
    "                logodds[..., None], posterior_natural_parameters)\n",
    "        )(plt_ps[..., None])\n",
    "    ),\n",
    "    label='my implementation'\n",
    ")\n",
    "\n",
    "axs[1].plot(plt_ps, jax.scipy.stats.beta.pdf(plt_ps, a + data.sum(), b + data.size - data.sum()),'--', label='scipy')\n",
    "axs[1].set_xlabel(r\"$p$\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task B.** \n",
    "\n",
    "Add a `predictive_log_marginal_pdf(x, natural_parameters)` function to the `ExponentialFamily` above (a placeholder has already been included). It should compute\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\log p(x \\mid \\alpha, \\nu) = \\log \\int_\\mathbb{W} p(x \\mid w) p(w \\mid \\alpha, \\nu) \\mathrm{d}w.\n",
    "\\end{equation*}\n",
    "This can be explicitly implemented in the abstract base class if the `conjugate_log_partition` is available. Revisit slide 10 of Lecture 5 for reference.\n",
    "\n",
    "In fact, it is still possible to provide this functionality **approximately** even if `conjugate_log_partition` is *not* available, using the Laplace approximation. Add a `Laplace_predictive_log_marginal_pdf(self,x,natural_parameters, mode)` function to `ExponentialFamily`, which approximates the functionality of `predictive_log_marginal_pdf` when given a `mode` $w*=\\operatorname{arg\\,max}_w p(w\\mid \\alpha,\\nu)$ (compare with the `laplace_precision` function already in `ConjugateFamily`). Revisit slide 7 of Lecture 6 for reference.\n",
    "\n",
    "Test your implementation for the concrete example of the Binomial above (for the binomial, this marginal is known as the [Beta-Binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution) distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_mode(conjugate_natural_parameters):\n",
    "    \"\"\"Closed-form expression for the mode of the conjugate exponential family of the\n",
    "    log-odds parametrized Binomial distribution.\"\"\"\n",
    "    return jnp.atleast_1d(\n",
    "        jnp.log(\n",
    "            conjugate_natural_parameters[0]\n",
    "            / (conjugate_natural_parameters[1] - conjugate_natural_parameters[0])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.predictive_log_marginal_pdf(\n",
    "            [[0], [1]],\n",
    "            posterior_natural_parameters,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.Laplace_predictive_log_marginal_pdf(\n",
    "            [[0], [1]],\n",
    "            posterior_natural_parameters,\n",
    "            conjugate_mode(posterior_natural_parameters),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.logpdf(\n",
    "            [[0], [1]],\n",
    "            conjugate_mode(posterior_natural_parameters),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to submit your work:\n",
    "\n",
    "Export your answer into a pdf (for example using jupyter's `Save and Export Notebook as` feature in the `File` menu). Make sure to include all outputs, in particular plots. Also include your answer to the theory question, either by adding it as LaTeX code directly in the notebook, or by adding it as an extra page (e.g. a scan) to the pdf. Submit the exercise on Ilias, in the associated folder. **Do not forget to add your name(s) and matrikel number(s) above!)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
