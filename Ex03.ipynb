{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "<div style=\"text-align: right\"> University of Tübingen, Summer Term 2023  &copy; 2023 P. Hennig </div>\n",
    "\n",
    "## Exercise Sheet No. 3 — Exponential Families\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Submission by:\n",
    "* FirstName1, Surname1, Matrikelnummer: MatrikelnummerOfFirstTeamMember\n",
    "* FirstName2, Surname2, Matrikelnummer: MatrikelnummerOfSecondTeamMember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from tueplots import bundles\n",
    "from tueplots.constants.color import rgb\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({\"figure.dpi\": 200})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 (Coding Exercise)\n",
    "\n",
    "Consider the abstract base class `ExponentialFamily` introduced in the lecture (reproduced below for easy reference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import functools\n",
    "\n",
    "\n",
    "class ExponentialFamily(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def sufficient_statistics(self, x: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_base_measure(self, x: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_partition(self, parameters: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\"\"\"\n",
    "\n",
    "    def parameters_to_natural_parameters(\n",
    "        self, parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->(P)`\n",
    "        In some EF's, the canonical parameters are\n",
    "        actually a transformation of the natural parameters.\n",
    "        In such cases, this method should be overwritten to\n",
    "        provide the inverse transformation.\n",
    "        \"\"\"\n",
    "        return jnp.asarray(parameters)\n",
    "\n",
    "    def logpdf(\n",
    "        self, x: ArrayLike | jnp.ndarray, parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        log p(x|parameters)\n",
    "            = log h(x) + sufficient_statistics(x) @ natural_parameters - log Z(natural_parameters)\n",
    "            = log base measure + linear term - log partition\n",
    "        \"\"\"\n",
    "\n",
    "        x = jnp.asarray(x)\n",
    "        log_base_measure = self.log_base_measure(x)\n",
    "        natural_parameters = self.parameters_to_natural_parameters(parameters)\n",
    "        linear_term = (\n",
    "            self.sufficient_statistics(\n",
    "                x)[..., None, :] @ natural_parameters[..., None]\n",
    "        )[..., 0, 0]\n",
    "        log_partition = self.log_partition(parameters)\n",
    "\n",
    "        return log_base_measure + linear_term - log_partition\n",
    "\n",
    "    def conjugate_log_partition(\n",
    "        self, alpha: ArrayLike | jnp.ndarray, nu: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"The log partition function of the conjugate exponential family.\n",
    "        Signature `(P),()->()`\n",
    "        If(!) this is available, it allows analytic construction of the conjugate prior (and thus analytic posterior inference).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def conjugate_prior(self) -> \"ConjugateFamily\":\n",
    "        return ConjugateFamily(self)\n",
    "\n",
    "    def predictive_log_marginal_pdf(\n",
    "        self,\n",
    "        x: ArrayLike | jnp.ndarray,\n",
    "        conjugate_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\" Signature `(D),(P)->()`\n",
    "            log p(x|conjugate_natural_parameters)\n",
    "            Your answer to Part B below should be implemented here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def Laplace_predictive_log_marginal_pdf(\n",
    "        self,\n",
    "        x: ArrayLike | jnp.ndarray,\n",
    "        conjugate_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        mode: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\" Signature `(D),(P)->()`\n",
    "            log p(x|conjugate_natural_parameters)\n",
    "            Your answer to Part B below should be implemented here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def posterior_parameters(\n",
    "        self,\n",
    "        prior_natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        data: ArrayLike | jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Computes the natural parameters of the posterior distribution under the\n",
    "        conjugate prior.\n",
    "        Signature `(P),(D)->(P)`\n",
    "        This can be implemented already in the abc and inherited by all subclasses,\n",
    "        even if the conjugate log partition function is not available.\n",
    "        (In the latter case, only the unnormalized posterior is immediately available, see below).\n",
    "        \"\"\"\n",
    "\n",
    "        prior_natural_parameters = jnp.asarray(prior_natural_parameters)\n",
    "\n",
    "        sufficient_statistics = self.sufficient_statistics(data)\n",
    "\n",
    "        n = sufficient_statistics[..., 0].size\n",
    "        expected_sufficient_statistics = jnp.sum(\n",
    "            sufficient_statistics,\n",
    "            axis=tuple(range(sufficient_statistics.ndim)),\n",
    "        )\n",
    "\n",
    "        alpha_prior, nu_prior = (\n",
    "            prior_natural_parameters[:-1],\n",
    "            prior_natural_parameters[-1],\n",
    "        )\n",
    "\n",
    "        return jnp.append(alpha_prior + expected_sufficient_statistics, nu_prior + n)\n",
    "\n",
    "\n",
    "class ConjugateFamily(ExponentialFamily):\n",
    "    def __init__(self, likelihood: ExponentialFamily) -> None:\n",
    "        self._likelihood = likelihood\n",
    "\n",
    "    @functools.partial(jnp.vectorize, excluded={0}, signature=\"(d)->(p)\")\n",
    "    def sufficient_statistics(self, w: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\n",
    "        the sufficient statistics of the conjugate family are\n",
    "        the natural parameters and the (negative) log partition function of the likelihood.\n",
    "        \"\"\"\n",
    "        return jnp.append(\n",
    "            self._likelihood.parameters_to_natural_parameters(w),\n",
    "            -self._likelihood.log_partition(w),\n",
    "        )\n",
    "\n",
    "    def log_base_measure(self, w: ArrayLike | jnp.ndarray, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\n",
    "        the base measure of the conjugate family is, implicitly, the Lebesgue measure.\n",
    "        \"\"\"\n",
    "        w = jnp.asarray(w)\n",
    "\n",
    "        return jnp.zeros_like(w[..., 0])\n",
    "\n",
    "    def log_partition(\n",
    "        self, natural_parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\n",
    "        If the conjugate log partition function is available,\n",
    "        we can use it to compute the log partition function of the conjugate family.\n",
    "        \"\"\"\n",
    "        natural_parameters = jnp.asarray(natural_parameters)\n",
    "\n",
    "        alpha, nu = natural_parameters[:-1], natural_parameters[-1]\n",
    "\n",
    "        return self._likelihood.conjugate_log_partition(alpha, nu)\n",
    "\n",
    "    def unnormalized_logpdf(\n",
    "        self, w: ArrayLike | jnp.ndarray, natural_parameters: ArrayLike | jnp.ndarray, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        Even if the conjugate log partition function is not available,\n",
    "        we can still compute the unnormalized log pdf of the conjugate family.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.sufficient_statistics(w) @ jnp.asarray(natural_parameters)\n",
    "\n",
    "    def laplace_precision(\n",
    "        self,\n",
    "        natural_parameters: ArrayLike | jnp.ndarray,\n",
    "        mode: ArrayLike | jnp.ndarray,\n",
    "        /,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P),(D)->()`\n",
    "        If the conjugate log partition function is _not_ available,\n",
    "        we can still compute the Laplace approximation to the posterior,\n",
    "        using only structure provided by the likelihood.\n",
    "        This requires the mode of the likelihood, which is not available in general,\n",
    "        but may be found by numerical optimization if necessary.\n",
    "        \"\"\"\n",
    "        return -jax.hessian(self.unnormalized_logpdf, argnums=0)(\n",
    "            jnp.asarray(mode), natural_parameters\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task A.** \n",
    "\n",
    "Implement a concrete realization of the binomial exponential family parametrized by log odds ratio $w = \\log \\frac{p}{1 - p}$, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "    p(k \\mid w) = \\exp \\left(\\log h(k) + \\phi(k)^T w - \\log Z(w) \\right),\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "* $\\log h(k) := \\log \\binom{n}{k}$,\n",
    "* $\\phi(k) := k$, and\n",
    "* $\\log Z(w) := n \\log (1 + \\exp(w))$.\n",
    "\n",
    "(Note that $n$ is a constant in this definition, not a parameter). The normalization constant of the conjugate family\n",
    "\n",
    "\\begin{align*}\n",
    "    F(\\alpha, \\nu)\n",
    "    & := \\int_{-\\infty}^\\infty \\exp \\left( \\alpha w - \\nu \\log Z(w) \\right) \\mathrm{d}w \\\\\n",
    "    & = \\int_{-\\infty}^\\infty \\exp \\left( w \\right)^\\alpha \\left( 1 + \\exp(w) \\right)^{-n \\nu} \\mathrm{d}w \\\\\n",
    "    & = \\int_0^1 \\left( \\frac{p}{1 - p} \\right)^\\alpha \\left( 1 + \\frac{p}{1 - p} \\right)^{-n \\nu} \\left| \\frac{1}{p (1 - p)} \\right| \\mathrm{d}p \\\\\n",
    "    & = \\int_0^1 p^{\\alpha - 1} (1 - p)^{(n \\nu - \\alpha) - 1} \\mathrm{d}p \\\\\n",
    "    & = B(\\alpha, n \\nu - \\alpha),\n",
    "\\end{align*}\n",
    "\n",
    "since $p = \\frac{1}{1 + \\exp(-w)}$ and $\\frac{\\mathrm{d} p}{\\mathrm{d} w} = \\frac{\\exp(-w)}{(1 + \\exp(-w))^2} = p (1 - p)$.\n",
    "This is also the normalization constant of the type VI logistic or logistic-beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus, the following transformation is a useful utility:\n",
    "def sigmoid_logpdf_transform(logpdf_logodds):\n",
    "    \"\"\"Transform the log-pdf of a random variable X into the\n",
    "    log-pdf of the random variable sigmoid(X)\"\"\"\n",
    "\n",
    "    def logpdf_p(ps):\n",
    "        logps = jnp.log(ps)\n",
    "        log1mps = jnp.log1p(-ps)\n",
    "        logodds = logps - log1mps\n",
    "\n",
    "        return logpdf_logodds(logodds) - logps - log1mps\n",
    "\n",
    "    return logpdf_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your implementation of the Binomial distribution ###\n",
    "class BinomialLogOdds(ExponentialFamily):\n",
    "    \"\"\"The Binomial distribution\"\"\"\n",
    "    \n",
    "    def __init__(self, n: int) -> None:\n",
    "        \"\"\"The Binomial distribution needs a fixed N\"\"\"\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def sufficient_statistics(self, k: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"The sufficient statistics are the identity function.\"\"\"\n",
    "        return jnp.asarray(k)\n",
    "    \n",
    "    def log_base_measure(self, k: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"h(k)=n!/(k!(n-k)!), thus log h(k) = log n! - log (k!(n-k)!)\"\"\"\n",
    "        k = jnp.asarray(k)\n",
    "        return jax.scipy.special.gammaln(self.n + 1) -jax.scipy.special.gammaln(k[...,0] + 1) - jax.scipy.special.gammaln(self.n - k[...,0] + 1)\n",
    "\n",
    "    def log_partition(self, w: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"log Z(w) = N log 1+exp(w)\"\"\"\n",
    "        w = jnp.asarray(w)\n",
    "        return self.n * jnp.log(1 + jnp.exp(w[...,0]))\n",
    "    \n",
    "    def parameters_to_natural_parameters(self, w: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"eta = log(p/1-p) = log(p) - log(1-p)\"\"\"\n",
    "        w = jnp.asarray(w)\n",
    "        p = 1 / (1 + jnp.exp(-w[...,0]))\n",
    "        return jnp.log(w) - jnp.log(1-w)\n",
    "    \n",
    "    def conjugate_log_partition(self, alpha: ArrayLike | jnp.ndarray, nu: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"log Z(alpha, nu) = Gamma(alpha) + Gamma(nu) - Gamma(alpha + nu)\"\"\"\n",
    "        return jax.scipy.special.gammaln(alpha+1) + jax.scipy.special.gammaln(nu+1) - jax.scipy.special.gammaln(alpha + nu + 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_18268\\3658829949.py:27: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  scipy.integrate.quad(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-05, atol=0\nThe conjugate prior is not correctly normalized.\nx and y nan location mismatch:\n x: array(nan)\n y: array(1.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# A: Check your implementation of the conjugate prior is correctly normalized:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mintegrate\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_allclose(\n\u001b[0;32m     27\u001b[0m     scipy\u001b[39m.\u001b[39;49mintegrate\u001b[39m.\u001b[39;49mquad(\n\u001b[0;32m     28\u001b[0m         \u001b[39mlambda\u001b[39;49;00m logodds: np\u001b[39m.\u001b[39;49mexp(prior\u001b[39m.\u001b[39;49mlogpdf(\n\u001b[0;32m     29\u001b[0m             [logodds], prior_natural_parameters)),\n\u001b[0;32m     30\u001b[0m         \u001b[39m-\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     31\u001b[0m         \u001b[39m30\u001b[39;49m,\n\u001b[0;32m     32\u001b[0m     )[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m     33\u001b[0m     \u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m     34\u001b[0m     rtol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m,\n\u001b[0;32m     35\u001b[0m     err_msg\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mThe conjugate prior is not correctly normalized.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[39m# B: check your log pdf against the scipy implementation:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m, sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, sharey\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\miche\\anaconda3\\envs\\prob_ML\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\miche\\anaconda3\\envs\\prob_ML\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:753\u001b[0m, in \u001b[0;36massert_array_compare.<locals>.func_assert_same_pos\u001b[1;34m(x, y, func, hasval)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m bool_(x_id \u001b[39m==\u001b[39m y_id)\u001b[39m.\u001b[39mall() \u001b[39m!=\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    749\u001b[0m     msg \u001b[39m=\u001b[39m build_err_msg([x, y],\n\u001b[0;32m    750\u001b[0m                         err_msg \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mx and y \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m location mismatch:\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    751\u001b[0m                         \u001b[39m%\u001b[39m (hasval), verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m    752\u001b[0m                         names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[1;32m--> 753\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[0;32m    754\u001b[0m \u001b[39m# If there is a scalar, then here we know the array has the same\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[39m# flag as it everywhere, so we should return the scalar flag.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x_id, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m x_id\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-05, atol=0\nThe conjugate prior is not correctly normalized.\nx and y nan location mismatch:\n x: array(nan)\n y: array(1.)"
     ]
    }
   ],
   "source": [
    "# Some unit tests to make sure your implementation is correct:\n",
    "# instantiate your EF, and its conjugate prior:\n",
    "likelihood = BinomialLogOdds(n=1)\n",
    "prior = likelihood.conjugate_prior()\n",
    "\n",
    "a, b = 0.5, 0.5\n",
    "prior_natural_parameters = [\n",
    "    a,  # alpha\n",
    "    a + b,  # nu\n",
    "]  # => Logistic-Beta(a, b)\n",
    "\n",
    "# create some data:\n",
    "key = jax.random.PRNGKey(0)\n",
    "data = jax.random.bernoulli(key, 0.75, shape=(20, 1))\n",
    "\n",
    "posterior = prior\n",
    "posterior_natural_parameters = likelihood.posterior_parameters(\n",
    "    prior_natural_parameters,\n",
    "    data,\n",
    ")\n",
    "\n",
    "\n",
    "# A: Check your implementation of the conjugate prior is correctly normalized:\n",
    "import scipy.integrate\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    scipy.integrate.quad(\n",
    "        lambda logodds: np.exp(prior.logpdf(\n",
    "            [logodds], prior_natural_parameters)),\n",
    "        -30,\n",
    "        30,\n",
    "    )[0],\n",
    "    1.0,\n",
    "    rtol=1e-5,\n",
    "    err_msg=\"The conjugate prior is not correctly normalized.\",\n",
    ")\n",
    "\n",
    "# B: check your log pdf against the scipy implementation:\n",
    "fig, axs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "plt_ps = np.linspace(0.0, 1.0, 100)\n",
    "\n",
    "# first for the prior:\n",
    "axs[0].plot(\n",
    "    plt_ps,\n",
    "    jnp.exp(\n",
    "        sigmoid_logpdf_transform(\n",
    "            lambda logodds: prior.logpdf(\n",
    "                logodds[..., None], prior_natural_parameters)\n",
    "        )(plt_ps[..., None])\n",
    "    ), \n",
    "    label='my implementation'\n",
    ")\n",
    "\n",
    "axs[0].plot(plt_ps, jax.scipy.stats.beta.pdf(plt_ps, a, b),'--', label='scipy')\n",
    "axs[0].set_xlabel(r\"$p$\")\n",
    "\n",
    "# then for the posterior:\n",
    "axs[1].plot(\n",
    "    plt_ps,\n",
    "    jnp.exp(\n",
    "        sigmoid_logpdf_transform(\n",
    "            lambda logodds: posterior.logpdf(\n",
    "                logodds[..., None], posterior_natural_parameters)\n",
    "        )(plt_ps[..., None])\n",
    "    ),\n",
    "    label='my implementation'\n",
    ")\n",
    "\n",
    "axs[1].plot(plt_ps, jax.scipy.stats.beta.pdf(plt_ps, a + data.sum(), b + data.size - data.sum()),'--', label='scipy')\n",
    "axs[1].set_xlabel(r\"$p$\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task B.** \n",
    "\n",
    "Add a `predictive_log_marginal_pdf(x, natural_parameters)` function to the `ExponentialFamily` above (a placeholder has already been included). It should compute\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\log p(x \\mid \\alpha, \\nu) = \\log \\int_\\mathbb{W} p(x \\mid w) p(w \\mid \\alpha, \\nu) \\mathrm{d}w.\n",
    "\\end{equation*}\n",
    "This can be explicitly implemented in the abstract base class if the `conjugate_log_partition` is available. Revisit slide 10 of Lecture 5 for reference.\n",
    "\n",
    "In fact, it is still possible to provide this functionality **approximately** even if `conjugate_log_partition` is *not* available, using the Laplace approximation. Add a `Laplace_predictive_log_marginal_pdf(self,x,natural_parameters, mode)` function to `ExponentialFamily`, which approximates the functionality of `predictive_log_marginal_pdf` when given a `mode` $w*=\\operatorname{arg\\,max}_w p(w\\mid \\alpha,\\nu)$ (compare with the `laplace_precision` function already in `ConjugateFamily`). Revisit slide 7 of Lecture 6 for reference.\n",
    "\n",
    "Test your implementation for the concrete example of the Binomial above (for the binomial, this marginal is known as the [Beta-Binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution) distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_mode(conjugate_natural_parameters):\n",
    "    \"\"\"Closed-form expression for the mode of the conjugate exponential family of the\n",
    "    log-odds parametrized Binomial distribution.\"\"\"\n",
    "    return jnp.atleast_1d(\n",
    "        jnp.log(\n",
    "            conjugate_natural_parameters[0]\n",
    "            / (conjugate_natural_parameters[1] - conjugate_natural_parameters[0])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.predictive_log_marginal_pdf(\n",
    "            [[0], [1]],\n",
    "            posterior_natural_parameters,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.Laplace_predictive_log_marginal_pdf(\n",
    "            [[0], [1]],\n",
    "            posterior_natural_parameters,\n",
    "            conjugate_mode(posterior_natural_parameters),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    [0, 1],\n",
    "    np.exp(\n",
    "        likelihood.logpdf(\n",
    "            [[0], [1]],\n",
    "            conjugate_mode(posterior_natural_parameters),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plt.xticks([0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to submit your work:\n",
    "\n",
    "Export your answer into a pdf (for example using jupyter's `Save and Export Notebook as` feature in the `File` menu). Make sure to include all outputs, in particular plots. Also include your answer to the theory question, either by adding it as LaTeX code directly in the notebook, or by adding it as an extra page (e.g. a scan) to the pdf. Submit the exercise on Ilias, in the associated folder. **Do not forget to add your name(s) and matrikel number(s) above!)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
